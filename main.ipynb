{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importation et création du dictionnaire des classes** \n",
    "\n",
    "Il faudra tout d'abords commencer par importer et définir les classes utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import natsort\n",
    "from functools import reduce \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "classes = { \n",
    "    0:\"Limitation de vitesse (20km/h)\",\n",
    "    1:\"Limitation de vitesse (30km/h)\", \n",
    "    2:\"Limitation de vitesse (50km/h)\", \n",
    "    3:\"Limitation de vitesse (60km/h)\", \n",
    "    4:\"Limitation de vitesse (70km/h)\", \n",
    "    5:\"Limitation de vitesse (80km/h)\", \n",
    "    6:\"Fin de limitation de vitesse (80km/h)\", \n",
    "    7:\"Limitation de vitesse (100km/h)\", \n",
    "    8:\"Limitation de vitesse (120km/h)\", \n",
    "    9:\"Interdiction de depasser\", \n",
    "    10:\"Interdiction de depasser pour vehicules > 3.5t\", \n",
    "    11:\"Intersection ou' vous etes prioritaire\", \n",
    "    12:\"Route prioritaire\", \n",
    "    13:\"Ceder le passage\", \n",
    "    14:\"Arret a' l'intersection\", \n",
    "    15:\"Circulation interdite\", \n",
    "    16:\"Acces interdit aux vehicules > 3.5t\", \n",
    "    17:\"Sens interdit\", \n",
    "    18:\"Danger\", \n",
    "    19:\"Virage a' gauche\", \n",
    "    20:\"Virage a' droite\", \n",
    "    21:\"Succession de virages\", \n",
    "    22:\"Cassis ou dos-d'ane\", \n",
    "    23:\"Chaussee glissante\", \n",
    "    24:\"Chaussee retrecie par la droite\", \n",
    "    25:\"Travaux en cours\", \n",
    "    26:\"Annonce feux\", \n",
    "    27:\"Passage pietons\", \n",
    "    28:\"Endroit frequente' par les enfants\", \n",
    "    29:\"Debouche' de cyclistes\", \n",
    "    30:\"Neige ou glace\",\n",
    "    31:\"Passage d'animaux sauvages\", \n",
    "    32:\"Fin des interdictions precedemment signalees\", \n",
    "    33:\"Direction obligatoire a' la prochaine intersection : a' droite\", \n",
    "    34:\"Direction obligatoire a' la prochaine intersection : a' gauche\", \n",
    "    35:\"Direction obligatoire a' la prochaine intersection : tout droit\", \n",
    "    36:\"Direction obligatoire a' la prochaine intersection : tout droit ou a' droite\", \n",
    "    37:\"Direction obligatoire a' la prochaine intersection : tout droit ou a' gauche\", \n",
    "    38:\"Contournement obligatoire de l'obstacle par la droite\", \n",
    "    39:\"Contournement obligatoire de l'obstacle par la gauche\", \n",
    "    40:\"Carrefour giratoire\", \n",
    "    41:\"Fin d'interdiction de depasser\", \n",
    "    42:\"Fin d'interdiction de depasser pour vehicules > 3.5t\" \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Préparation des données**\n",
    " \n",
    "Les datasets d'entrainement/validation seront préparés à l'aide de ImageFolder car les données sont organisées correctement pour cette fonction de création de Dataset.   \n",
    " \n",
    "Etant donné que les images passées aux modèles ont des tailles différentes, il est nécessaire d'appliquer à chaque fois une transformation pour transformer les images en tenseurs de taille 32x32, ce qui correspond aux dimensions d'input du modèle.  \n",
    " \n",
    "De plus, ImageFolder associe les labels aux noms des dossiers à l'aide d'un dictionnaire, il faudra donc appliquer une transformation sur ces nouveaux labels afin de restaurer les labels originaux.  \n",
    "   \n",
    "Un split est effectué sur le dataset d'ImageFolder initial afin d'obtenir un ensemble d'entraînement et un ensemble de validation avec une répartition 80/20. Le dataset de validation ne sera jamais utilisé pour s'entraîner et servira uniquement à juger les performances du modèle lors de son entraînement.\n",
    " \n",
    "Une fois ces deux nouveaux datasets créés, il seront chacuns chargés dans un DataLoader. La taille des batchs du loader d'entraînement seront légèrement plus petites afin d'éviter de passer des batchs trop gros lors de l'entraînement, et ces batchs seront shuffle afin d'éviter que le modèle apprend toujours sur les données dans le même ordre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length size : 39210\n",
      "Train dataset length size : 7842\n"
     ]
    }
   ],
   "source": [
    "#Resize images to 32x32\n",
    "trans = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((32,32)),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "#Path to Dataset\n",
    "trainData = torchvision.datasets.ImageFolder(\"panneaux_route/Train\", transform=trans)\n",
    "\n",
    "#Restore original labels\n",
    "trainData.target_transform = lambda id: int(trainData.classes[id])\n",
    "\n",
    "#Division du dataset en train et validation\n",
    "trainSize = int(0.8 * len(trainData))\n",
    "validationSize = len(trainData) - trainSize\n",
    "trainDataset, validationDataset = random_split(trainData, [trainSize, validationSize])\n",
    "print(\"Train dataset length size :\",len(trainData))\n",
    "print(\"Train dataset length size :\",len(validationDataset))\n",
    "\n",
    "#Chargement dans un dataloader\n",
    "trainLoader = DataLoader(trainDataset, batch_size=256, shuffle=True)\n",
    "validationLoader = DataLoader(validationDataset, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Création et choix du modèle**\n",
    " \n",
    "Le modèle utilisé est similaire à celui du TP8 et est basé sur LeNet5.  \n",
    "   \n",
    "Le modèle implémente les fonctionnalitées suivantes :  \n",
    " - Dropout (0.5)  \n",
    " - Batch Normalisation(0.0002)  \n",
    " \n",
    "Ces deux méthodes permettent de réduire l'overfitting, ce qui améliore légerement la précision sur l'ensemble de validation et de test. Le gain le plus conséquent est celui apporté par le dropout avec environ 3-4% de gain en précision.  \n",
    " \n",
    "Des couches supplémentaires de convolutions ont été ajoutées comme sur le TP8 afin d'augmenter la complexité du modèle sans modifier les dimensions d'entrées ou de sortie des couches intermédiaires.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self,dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3, 3))    \n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3))\n",
    "        self.dropout1 = nn.Dropout(p=dropout_p)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 3))    \n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=(3, 3)) \n",
    "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=(3, 3))    \n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=(3, 3)) \n",
    "        self.dropout3 = nn.Dropout(p=dropout_p)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 43)\n",
    "\n",
    "    def forward(self, input):\n",
    "        layer1 = F.relu(self.conv1(input))                  \n",
    "        layer2 = F.relu(self.conv2(layer1))\n",
    "        layer3 = F.max_pool2d(layer2, kernel_size=2, stride=2)\n",
    "        layer3_d = self.dropout1(layer3)\n",
    "        layer3_db = self.batchnorm1(layer3_d)\n",
    "        layer4 = F.relu(self.conv3(layer3_db))                  \n",
    "        layer5 = F.relu(self.conv4(layer4))            \n",
    "        layer6 = F.max_pool2d(layer5, kernel_size=2, stride=2)\n",
    "        layer6_d = self.dropout2(layer6)\n",
    "        layer6_db = self.batchnorm2(layer6_d)\n",
    "        layer7 = F.relu(self.conv5(layer6_db))                  \n",
    "        layer8 = F.relu(self.conv6(layer7))\n",
    "        layer8_d = self.dropout3(layer8)     \n",
    "        layer8_db = self.batchnorm3(layer8_d)       \n",
    "        layer9 = F.relu(self.fc1(torch.flatten(layer8_db,1)))  \n",
    "        layer9_b = self.batchnorm4(layer9)\n",
    "        layer10 = self.fc2(layer9_b)                        \n",
    "        return layer10\n",
    "\n",
    "\n",
    "mainModel = LeNet5(dropout_p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nombre de paramètres du modèle**  \n",
    "  \n",
    "La complexité du modèle reste raisonnable et celui-ci prend environ 40-50s pour finir de s'entraîner sur une epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 32, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([43, 256])\n",
      "torch.Size([43])\n",
      "total nb parameters:  332043\n"
     ]
    }
   ],
   "source": [
    "total_nb_par = 0\n",
    "for p in mainModel.parameters():\n",
    "    print(p.shape)\n",
    "    total_nb_par += reduce(lambda x, y: x*y, p.shape, 1)\n",
    "print(\"total nb parameters: \", total_nb_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boucle d'entraînement**\n",
    "\n",
    "Notre boucle d'entraînement utilise une descente de gradient stochastique avec mini batch et implémente les fonctionnalitées suivantes :  \n",
    " - Weight decay (0.0002)  \n",
    " - Sauvegarde de la meilleure itération  \n",
    " \n",
    "La valeur du weight decay à été choisie en utilisant plusieurs valeurs différentes et en gardant celle qui apportait la meilleure précision sur l'ensemble de test.  \n",
    "  \n",
    "La sauvegarde de la meilleure itération remplace ici l'utilisation de l'early stop. A la fin de chaque epoch, la boucle d'entraînement sauvegarde les poids avec la meilleure précision sur l'ensemble de validation. Cette précision est obtenue à l'aide de notre fonction \"validate\", qui permet d'obtenir le score du modèle sur nos ensemble d'entraînement et de validation.  \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_loader, model, loss_map, lr=1e-3, epochs=20, weight_decay=0.0):\n",
    "    # use gpu if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # create optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    history=[]\n",
    "\n",
    "    bestModel = mainModel.state_dict()\n",
    "    bestValidationAcc = 0\n",
    "    # Train model\n",
    "    model.train() \n",
    "    for epoch in range(epochs):\n",
    "        loss_epoch = 0.\n",
    "        previousTime = time.time()\n",
    "        for images, labels in train_loader:\n",
    "            # Transfers data to GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Primal computation\n",
    "            output = model(images)            \n",
    "            loss = loss_map(output, labels)            \n",
    "            # Gradient computation\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            # perform parameter update based on current gradients\n",
    "            optimizer.step()\n",
    "            # compute the epoch training loss\n",
    "            loss_epoch += loss.item()\n",
    "        # display the epoch training loss\n",
    "        currentTime = time.time()\n",
    "\n",
    "        validationAccuracy = round(validate(validationLoader, model).item(),3)\n",
    "        trainingAccuracy = round(validate(train_loader, model).item(),3)\n",
    "\n",
    "        history.append({\"epoch\" : epoch, \"loss\" : loss_epoch, \"train_acc\" : trainingAccuracy, \"val_acc\" : validationAccuracy})\n",
    "        print(f\"epoch : {epoch + 1}/{epochs}, loss = {loss_epoch:.6f}, validation_accuracy = {validationAccuracy}% , train_accuracy = {trainingAccuracy}%, epoch_time = {(currentTime - previousTime):.1f}s, time_remaining = {((currentTime - previousTime)*(epochs - epoch)):.1f}s\")\n",
    "        if(validationAccuracy > bestValidationAcc):\n",
    "            bestModel = mainModel.state_dict()\n",
    "            bestValidationAcc = validationAccuracy\n",
    "            print(f\"New best model with {validationAccuracy}% accuracy on validation set !\")\n",
    "    \n",
    "    mainModel.load_state_dict(bestModel)\n",
    "        \n",
    "    return history\n",
    "    \n",
    "def show_history(history):\n",
    "    figure, axis = plt.subplots(1, 2)\n",
    "    axis[0].plot([d[\"epoch\"] for d in history], [d[\"loss\"] for d in history])\n",
    "    axis[0].set_title(\"Loss per epoch\")\n",
    "\n",
    "    axis[1].plot([d[\"epoch\"] for d in history], [d[\"train_acc\"] for d in history], label=\"train\")\n",
    "    axis[1].plot([d[\"epoch\"] for d in history], [d[\"val_acc\"] for d in history], label=\"validation\")\n",
    "    axis[1].set_title(\"Accuracy per epoch\")\n",
    " \n",
    "    plt.legend() \n",
    "    plt.show() \n",
    "     \n",
    "#Renvoie le taux de réussite d'un model sur un dataloader contenant un tuple (images,labels)         \n",
    "def validate(data_loader, model):\n",
    "    nb_errors = 0\n",
    "    nb_tests = 0\n",
    "    device = next(model.parameters()).device # current model device\n",
    "    model.eval()\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device) # move data same model device\n",
    "        output = model(images)\n",
    "        nb_errors += ((output.argmax(1)) != labels).sum()\n",
    "        nb_tests += len(images)\n",
    "    \n",
    "    return torch.div((100*(nb_tests-nb_errors)),nb_tests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution**\n",
    " \n",
    "Le modèle charge automatiquement un ensemble de poids déjà existant nommé \"modelWeights\" si celui-ci existe.\n",
    "Dans ce cas là, le modèle ne s'entraîne pas. Dans le cas contraire, celui-ci lance la boucle d'entraînement puis sauvegarde sur disque les poids obtenus après l'entraînement.\n",
    " \n",
    "Le nombre d'epochs utilisé reste raisonnable ici, mais il serait potentiellement possible d'améliorer encore la précision du modèle avec plus de temps étant donné que l'on sauvegarde toujours la meilleure itération lors de l'entraînement.\n",
    "  \n",
    "Après entraînement, on arrive a environ 99.5% de précision sur l'ensemble de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fnn before learning, accuracy = 3.672532558441162%\n",
      "epoch : 1/15, loss = 379.696723, validation_accuracy = 51.734% , train_accuracy = 51.973%, epoch_time = 48.8s, time_remaining = 732.3s\n",
      "New best model with 51.734% accuracy on validation set !\n",
      "epoch : 2/15, loss = 53.924402, validation_accuracy = 97.054% , train_accuracy = 97.59%, epoch_time = 43.6s, time_remaining = 610.8s\n",
      "New best model with 97.054% accuracy on validation set !\n",
      "epoch : 3/15, loss = 8.542366, validation_accuracy = 98.75% , train_accuracy = 99.334%, epoch_time = 43.7s, time_remaining = 568.4s\n",
      "New best model with 98.75% accuracy on validation set !\n",
      "epoch : 4/15, loss = 3.599239, validation_accuracy = 98.648% , train_accuracy = 99.321%, epoch_time = 46.5s, time_remaining = 558.4s\n",
      "epoch : 5/15, loss = 2.897272, validation_accuracy = 98.623% , train_accuracy = 99.509%, epoch_time = 45.4s, time_remaining = 499.2s\n",
      "epoch : 6/15, loss = 1.089940, validation_accuracy = 99.171% , train_accuracy = 99.866%, epoch_time = 45.8s, time_remaining = 458.5s\n",
      "New best model with 99.171% accuracy on validation set !\n",
      "epoch : 7/15, loss = 1.269063, validation_accuracy = 98.865% , train_accuracy = 99.334%, epoch_time = 45.9s, time_remaining = 413.2s\n",
      "epoch : 8/15, loss = 0.797915, validation_accuracy = 99.388% , train_accuracy = 99.943%, epoch_time = 45.9s, time_remaining = 367.0s\n",
      "New best model with 99.388% accuracy on validation set !\n",
      "epoch : 9/15, loss = 0.817329, validation_accuracy = 99.452% , train_accuracy = 99.908%, epoch_time = 45.8s, time_remaining = 320.9s\n",
      "New best model with 99.452% accuracy on validation set !\n",
      "epoch : 10/15, loss = 0.812616, validation_accuracy = 98.814% , train_accuracy = 99.704%, epoch_time = 45.9s, time_remaining = 275.1s\n",
      "epoch : 11/15, loss = 0.573871, validation_accuracy = 98.878% , train_accuracy = 99.742%, epoch_time = 46.4s, time_remaining = 232.0s\n",
      "epoch : 12/15, loss = 0.886070, validation_accuracy = 99.107% , train_accuracy = 99.802%, epoch_time = 46.1s, time_remaining = 184.2s\n",
      "epoch : 13/15, loss = 1.269993, validation_accuracy = 99.362% , train_accuracy = 99.962%, epoch_time = 45.9s, time_remaining = 137.6s\n",
      "epoch : 14/15, loss = 0.610808, validation_accuracy = 99.515% , train_accuracy = 99.981%, epoch_time = 45.8s, time_remaining = 91.7s\n",
      "New best model with 99.515% accuracy on validation set !\n",
      "epoch : 15/15, loss = 0.731628, validation_accuracy = 99.426% , train_accuracy = 99.933%, epoch_time = 45.9s, time_remaining = 45.9s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fnn after learning, accuracy = 99.4261703491211%\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "if(os.path.exists(\"modelWeights\")):   \n",
    "    mainModel.load_state_dict(torch.load(\"modelWeights\"))\n",
    "    print(\"Weights already exists\")\n",
    "    print(f\"Accuracy = {validate(validationLoader, mainModel)}%\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Fnn before learning, accuracy = {validate(validationLoader, mainModel)}%\")\n",
    "    history=train_loop(\n",
    "        train_loader=trainLoader, \n",
    "        model=mainModel, \n",
    "        loss_map=nn.CrossEntropyLoss(),\n",
    "        lr=0.0005,\n",
    "        epochs=15,\n",
    "        weight_decay=0.0002)\n",
    "    show_history(history)\n",
    "    print(f\"Fnn after learning, accuracy = {validate(validationLoader, mainModel)}%\")\n",
    "\n",
    "    print(\"Saving model\")\n",
    "    torch.save(mainModel.state_dict(), \"modelWeights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploitation du modèle entraîné**  \n",
    " \n",
    "Une fois le modèle entraîné, il nous faudra ajouter des fonctions permettant d'exploiter celui-ci facilement en traitant les images passées en entrée et en retournant directement la prédiction.  \n",
    " \n",
    "La fonction predictFolder permet de passer au modèle toutes les images contenues dans le dossier passé en argument.  \n",
    "Afin d'éviter les problèmes de mémoire si le dossier contient trop d'images, il faudra créer un dataLoader pour passer des batchs d'image. Ce dataLoader se basera sur une Dataset custom simple servant à récupérer l'ensemble des images dans le dossier.  \n",
    " \n",
    "La fonction predictImage donne simplement l'image passée en argument au modèle et renvoie la prédiction de celui-ci.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renvoie la prédiction de model sur un tenseur contenant l'image à prédire\n",
    "def predict(imageTensor, model):\n",
    "    model.eval() \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    imageTensor = imageTensor.to(device)\n",
    "    return model(imageTensor.view(-1,3,32,32)).argmax(1)\n",
    "\n",
    "def predictFolder(path):\n",
    "\n",
    "    #Dataset used for test data\n",
    "    class basicFolderDataset(Dataset):\n",
    "        def __init__(self, main_dir, transform):\n",
    "            self.main_dir = main_dir\n",
    "            self.transform = transform\n",
    "            self.list = []\n",
    "            for file in os.listdir(main_dir):\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):#All supported image formats here\n",
    "                    self.list.append(file)\n",
    "            self.list = natsort.natsorted(self.list)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.list)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_loc = os.path.join(self.main_dir, self.list[idx])\n",
    "            image = Image.open(img_loc).convert(\"RGB\")\n",
    "            tensor_image = self.transform(image)\n",
    "            return tensor_image\n",
    "\n",
    "    load = DataLoader(basicFolderDataset(path, transform=trans), batch_size=1024)\n",
    "    tens = None\n",
    "    for batch in load:\n",
    "        if tens is None:\n",
    "            tens = predict(batch,mainModel)\n",
    "        else:\n",
    "            tens = torch.cat((tens,predict(batch,mainModel)),0)\n",
    "    return tens\n",
    "\n",
    "def predictImage(path):\n",
    "    return predict(trans(Image.open(path).convert(\"RGB\")),mainModel).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prédictions réelle**\n",
    " \n",
    "Les fonctions précédentes vont donc nous permettre de tester notre modèle sur l'ensemble de test. Afin de comparer les résultats de nos prédictions avec les labels donnés dans Test.csv, on va créer nous même un fichier Results.csv contenant l'ensemble des labels donnés par predictFolder.\n",
    " \n",
    "Après entraînement, notre modèle peut espérer un taux de réussite d'environ 96.5% sur l'ensemble de test.\n",
    "La comparaison est faite à l'aide de pandas en se basant sur le fichier de résultats créé et le fichier .csv contenant les labels des tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now predicting Test set\n",
      "Correct = 12192 | Total = 12630 | 96.53206650831353 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Now predicting Test set\")\n",
    "\n",
    "f = open(\"Results.csv\",\"w\")\n",
    "\n",
    "f.write(\"class, translated\\n\")\n",
    "res = predictFolder(\"panneaux_route/Test\")\n",
    "for i in res:\n",
    "    f.write(str(i.item())+\",\"+classes.get(i.item())+\"\\n\")\n",
    "f.close()\n",
    "\n",
    "results = pd.read_csv(\"Results.csv\")['class']\n",
    "labels = pd.read_csv(\"panneaux_route/Test.csv\")['ClassId']\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(results)):\n",
    "    if(results[i] == labels[i]):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(\"Correct =\",correct,\"| Total =\",total,\"|\",(correct/total)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Limitation de vitesse (30km/h)\n"
     ]
    }
   ],
   "source": [
    "print(predictImage(\"panneau.jpg\"))\n",
    "print(classes.get(predictImage(\"panneau.jpg\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "On peut remarquer que la plupart des images dans le dataset d'entraînement proviennent d'une seule et même image ayant été légèrement modifiée. Par exemple, les 8 premières images pour les panneaux de label 0 sont les suivantes :  \n",
    "  \n",
    "![](panneaux_route/Train/0/00000_00000_00000.png) ![](panneaux_route/Train/0/00000_00000_00001.png) ![](panneaux_route/Train/0/00000_00000_00002.png) ![](panneaux_route/Train/0/00000_00000_00003.png) ![](panneaux_route/Train/0/00000_00000_00004.png) ![](panneaux_route/Train/0/00000_00000_00005.png) ![](panneaux_route/Train/0/00000_00000_00006.png) ![](panneaux_route/Train/0/00000_00000_00007.png)  \n",
    "  \n",
    "Évidemment, effectuer une telle transformation directement sur les données initiales provoque quelques problèmes. Il devient très difficile d'éviter l'overfitting, étant donné qu'il est pratiquement inévitable qu'une image de l'ensemble de validation ne soit pas basée sur une image existant aussi dans l'ensemble d'entraînement. De ce fait, nos performances sur l'ensemble de validation ne sont pas vraiment indicatives de nos performances réelles, ici, nous sommes à 99.5% de précision sur l'ensemble de validation, mais seulement 96.5% sur un test réel.  \n",
    "  \n",
    "Il serait donc intéressant d'aller voir sur quels panneaux notre modèle à donc des difficultées, par exemple, les 5 premières mauvaises prédiction de notre modèle sur l'ensemble de Test sont les suivantes :  \n",
    "\n",
    "<img src='panneaux_route/Test/00012.png' width=\"80\" height=\"80\">\n",
    "<img src='panneaux_route/Test/00039.png' width=\"80\" height=\"80\">\n",
    "<img src='panneaux_route/Test/00044.png' width=\"80\" height=\"80\">\n",
    "<img src='panneaux_route/Test/00049.png' width=\"80\" height=\"80\">\n",
    "<img src='panneaux_route/Test/00050.png' width=\"80\" height=\"80\">\n",
    "\n",
    "Et voici donc la prédiction de notre modèle pour chacun de ces panneaux :\n",
    "\n",
    "- **Neige ou glace** au lieu de **Chaussee glissante**\n",
    "- **Limitation de vitesse (60km/h)** au lieu de **Limitation de vitesse (120km/h)**\n",
    "- **Endroit frequente' par les enfants** au lieu de **Chaussee retrecie par la droite**\n",
    "- **Limitation de vitesse (20km/h)** au lieu de **Limitation de vitesse (120km/h)**\n",
    "- **Passage d'animaux sauvages** au lieu de **Travaux en cours**\n",
    "\n",
    "![](panneaux_route/Meta/30.png) ![](panneaux_route/Meta/3.png) ![](panneaux_route/Meta/28.png) ![](panneaux_route/Meta/0.png) ![](panneaux_route/Meta/31.png)\n",
    "  \n",
    "Comme on peut le constater, pour chaque erreur, les panneaux partagent beaucoup de traits communs.  \n",
    "De plus, le fait d'orienter ou d'avoir une résolution trop faible diminue aussi grandement le taux de réussite de notre modèle."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e2b4823a3479359b1ac0e292467c6c8b83c17c3ed6275aa2673303cca00b390"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
